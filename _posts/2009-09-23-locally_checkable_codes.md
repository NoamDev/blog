---
id: 209
title: "קודים לתיקון שגיאות (שניתנים לבדיקה מקומית)"
date: 2009-09-23 19:21:09
layout: post
categories: 
  - מבני נתונים ואלגוריתמים
  - תורת הסיבוכיות
tags: 
  - קוד הדמר
  - קודים הניתנים לבדיקה מקומית
  - קודים לתיקון שגיאות
---
אוהבים ללעוג למתמטיקאים על כך שהמתמטיקה הנקייה והאידאלית שלהם לא מסתדרת טוב עם העולם האמיתי המלוכלך. גם מדעי המחשב סובלים מבעיה דומה, והראיתי את זה לא מזמן, ב<a href="http://www.gadial.net/2009/07/22/bad_math_rsa/">פוסט</a> שעסק במאמר שלעג ל"חשיבה המתמטית" שמונעת את ראיית הדרך ה"פשוטה" לפצח את RSA; טענתי אז שגם התאורטיקנים הגדולים ביותר יודעים לטפל - בואפן מתמטי ומדוייק - בבעיות שקשורות לעולם האמיתי. דוגמה מעניינת נוספת לכך היא <a href="http://gadial.blogli.co.il/wp-admin/%D7%A7%D7%95%D7%93%D7%99%D7%9D%20%D7%9C%D7%AA%D7%99%D7%A7%D7%95%D7%9F%20%D7%A9%D7%92%D7%99%D7%90%D7%95%D7%AAhttp://he.wikipedia.org/wiki/%D7%A7%D7%95%D7%93_%D7%AA%D7%99%D7%A7%D7%95%D7%9F_%D7%A9%D7%92%D7%99%D7%90%D7%95%D7%AA">קודים לתיקון שגיאות</a> - הם באים לטפל בבעיה אמיתית מאוד, ומשתמשים בהם כל הזמן בעולם האמיתי, ומצד שני התורה שלהם יכולה להיות תאורטית מאוד, מתבססת בחלקה הגדול על אלגברה לינארית ומופשטת, והחשוב מכל - יש לה שימושים מפתיעים גם בהוכחת תוצאות תיאורטיות לחלוטין בתחומים אחרים של מדעי המחשב, ובפרט את משפט ה-PCP ש<a href="http://www.gadial.net/2009/09/19/pcp_theorem_intro/">הזכרתי בפוסט הקודם</a>.

נחזור לבסיס. במדעי המחשב, אפשר לחשוב על כל אינפורמציה כאילו היא מיוצגת כרצף של סיביות - אפסים ואחדים. גם כשמאחסנים מידע במחשב ועל מדיות שונות ומשונות (כגון דיסקים) וגם כששולחים אותו בקו תקשורת, זה מה שנשלח - אפסים ואחדים. כמובן שבפועל מה שקורה הוא מסובך יותר, אבל זוהי הפשטה שאפשר לבצע ועדיין לקבל תיאור די מדוייק של המציאות; לא ארחיב בנושא מעבר לכך כעת.

אלא שהעולם האמיתי הוא מקום מלוכלך - לקווי תקשורת יש "רעש", דיסקים יכולים להישרט, וכן הלאה. בפועל המשמעות של זה מבחינתנו היא שחלק מהמידע שאנחנו שולחים (מכאן ואילך "לשלוח" פירושו גם "לשים מידע על דיסק"; תחשבו על זה כאילו אנחנו שולחים כך מידע אל גרסה עתידית שלנו שתנסה לקרוא את הדיסק) ייהרס. מה זה "ייהרס" מבחינתנו אם כל מה שיש הוא אפסים ואחדים? שבמקום לקרוא 0 נקרא 1, ולהפך. למשל, נשלח את המילה 0010 ובצעד השני תתקבל המילה 0110. כאן הביט השני "התקלקל". זה יכול לגרום לשינוי עצום במשמעות של המידע שנשלח ולהיות בעל תוצאות הרסניות - בקיצור, אסור להתיר לדברים כאלו לקרות. הבעיה היא שבעולם האמיתי הם קורים כל הזמן. לא סביר לבנות ערוץ ממשי שבו תקלות כאלו לא יתרחשו כל הזמן. בקצרה - אנו נזקקים לפתרון לבעיה שאינו תלוי ערוץ; אנחנו צריכים לקבל את הקלקולים בתור חלק מהחיים ולמצוא דרך לטפל בהם בכל זאת.

גישה נאיבית לבעיה היא זו - במקום לשדר את המידע סיבית-סיבית, נשכפל כל סיבית מספר פעמים כלשהו - נאמר, שלוש - ונשלח את השכפולים ברצף. כך למשל במקום לשלוח את המידע 0010, נשלח 000000111000. מי שמקבל את המידע בצד השני יפרק את הקלט שקיבל לשלשות, ויחליט מה הסיבית שכל שלשה מייצגת על פי "הכרעת רוב". למשל, אם קיבלנו 111, אפשר להניח שהשלשה הזו מייצגת את הסיבית 1; ואם קיבלנו 100, אפשר להניח שהשלשה מייצגת את הסיבית 0. כמובן, ייתכן שנשלח את 000 ובדרך כל שלוש הסיביות יתפקששו, בצד השני יתקבל 111 והמפענח יחשוב שניסיתי לשדר לו 1; אבל הסיכוי לכך שזה יקרה הוא נמוך יחסית. כמובן שכדי לבצע חישוב מדוייק צריך לדעת מהי ההסתברות לכך שסיבית תתפקשש, ובהתאם אפשר לשלוח 5 עותקים של הספרה שרוצים להעביר, או 7, וכו' - איני רוצה להיכנס לניתוחים הללו כעת.

הנה עוד דוגמה לדבר מה שניתן לעשות - אם אני רוצה לשלוח רצף של סיביות, נניח 01010, אני אוסיף לסוף הרצף עוד סיבית אחת, שתתקבל מ"חיבור" כל הסיביות (עם הכלל לפיו {% equation %}1+1=0{% endequation %}). הסיבית הזו נקראת "סיבית הזוגיות", שכן היא 0 אם יש מספר זוגי של 1 ברצף שאני שולח (למשל, ברצף שנתתי בדוגמה) והיא 1 אם יש מספר אי זוגי שלהן (למשל, ברצף 11100). בשביל מה זה טוב? אם בצד השני יקבלו 110001, יבינו שמשהו השתבש, כי סיבית הסימן היא 1 ועם זאת בין שאר חמש הסיביות (שאמורות לייצג את המידע האמיתי) יש רק שתיים שהן מגודל 1. זה טוב לזיהוי השגיאה, אבל לא ממש עוזר לתקן אותה במקרה הזה (כמובן שזיהוי שגיאה הוא חשוב מספיק לכשעצמו - אם שגיאות הן נדירות, אז כשמזוהה שגיאה אפשר לבקש מהשולח שישלח את פיסת המידע שוב).

באופן כללי אפשר לתאר קוד באמצעות שלושה פרמטרים - ראשית, מספר הסיביות שיש בכל מילת קוד; שנית, קבוצת המילים בקוד; שלישית, המרחק המינימלי שבין המילים בקוד. בואו נחזור לרגע לקוד השלשות שתיארתי קודם - זה קוד שהכיל בדיוק שתי מילות קוד, 111 ו-000; כל מילה אחרת הייתה "לא חוקית". אם כן, זה קוד שבו המילים הן מאורך 3 ויש בדיוק שתי מילים. הקוד הזה מאפשר לנו, אם כן, לשלוח שני "סוגי מידע שונים"; אני הגדרתי שרירותית ש-000 ייצג את 0 וש-111 ייצג את 1 אבל זה לא הכרחי; הייתי יכול גם לקבוע ש-000 מייצג את 1 ו-111 מייצג את 0, או שהם מייצגים בכלל את 012 ואת 4325, או כל זוג מוזר אחר. נותר להסביר מהו עניין ה"מרחק".

המילה 000 נראית לנו שונה מהמילה 111 הרבה יותר מאשר 110 שונה מ-111. ההבדל ברור -<strong> מספר המקומות השונים</strong> בין 000 ו-111 הוא 3, בעוד שמספר המקומות השונים בין 110 ו-111 הוא 1 בלבד. זה מוביל להגדרה של מרחק המינג בין שתי מחרוזות מאורך זהה - מספר המקומות השונים זה מזה במחרוזת, או אם תרצו: מספר <strong>השגיאות</strong> שצריכות להתרחש כדי שבמקום המחרוזת א' תתקבל המחרוזת ב'.

הסיבה שקוראים להגדרה הזו "<a href="http://he.wikipedia.org/wiki/%D7%9E%D7%98%D7%A8%D7%99%D7%A7%D7%94">מרחק</a>" היא שהתכונות הקלאסיות של מרחק מתקיימות עבורה. המרחק של כל מילה מעצמה הוא 0, ואם יש מרחק 0 בין שתי מילים, זוהי אותה המילה; המרחק של א' מב' זהה למרחק של ב' מא'; והמרחק של א' מג' קטן מסכום המרחקים של א' מב' ומב' לג', לכל מילה ב'. לתכונה האחרונה קוראים "אי שוויון המשולש" בגלל שהיא ניסוח פורמלי של התכונה המוכרת לפיה במשולש סכום אורכי שתי צלעות תמיד גדול או שווה לאורך הצלע השלישית. אם חושבים על צלעות המשולש בתור מסלולים שיש ללכת בהם, אי שוויון המשולש הוא פורמליזציה של האבחנה הפשוטה לפיה עדיף ללכת בקו ישר ולא להתעכב בנקודות ביניים בדרך.

אם כן, המרחק המינימלי בין שתי מילים בקוד הוא מספר השגיאות שצריכות ליפול במהלך השידור כדי שנטעה לחשוב ששום דבר לא השתבש, ונקבל מילת קוד לא נכונה. יותר מזה - <strong>חצי</strong> מהמרחק המינימלי הוא מספר השגיאות המקסימלי שיכולות ליפול בקוד כך שעדיין נוכל לשחזר את מילת הקוד הנכונה המקורית (למעשה, מספר השגיאות חייב להיות קטן <strong>ממש</strong> מחצי) - נסו לחשוב מדוע.

כעת אני רוצה לעזוב את הנושא הכללי הזה, שלא התחלתי אפילו לדגדג את קצה-קצהו, ולדבר על קוד מאוד ספציפי, עם תכונה מאוד ספציפית - <a href="http://en.wikipedia.org/wiki/Hadamard_code">קוד הדמר</a> (Hadamard), עם תכונה מאוד ספציפית - הוא <a href="http://en.wikipedia.org/wiki/Locally_testable_code">ניתן לבדיקה מקומית </a>(Locally Testable). הכוונה ב"בדיקה מקומית" היא שבאמצעות דגימה של מספר קטן מאוד - סופי - של ביטים ממילת הקוד ניתן יהיה לזהות בהסתברות טובה האם היא חוקית או לא. ליתר דיוק - אם היא חוקית, תמיד נזהה זאת; אם היא לא חוקית, אז ככל שהיא רחוקה מלהיות מילת קוד חוקית ההסתברות שלנו לזהות זאת תגדל משמעותית (מובן מאליו שאם נפלה שגיאה בביט בודד במילת הקוד אין סיכוי לזהות זאת בהסתברות גדולה על ידי בדיקה של מספר סופי של ביטים - הרי חייבים לקלוע "בול" לביט שהתקלקל כדי שיהיה בכלל סיכוי להבין שמשהו השתבש).

קוד הדמר, כמו רוב הקודים לתיקון שגיאות, הוא קוד לינארי - מילות הקוד מהוות <a href="http://he.wikipedia.org/wiki/%D7%9E%D7%A8%D7%97%D7%91_%D7%95%D7%A7%D7%98%D7%95%D7%A8%D7%99">מרחב לינארי</a> מעל <a href="http://he.wikipedia.org/wiki/%D7%A9%D7%93%D7%94_%D7%A1%D7%95%D7%A4%D7%99">שדה סופי</a> כלשהו. אני לא סתם נכנס לעובדה הטכנית הזו - כדי להבין את הרעיון בקוד אין כמעט מנוס מלהכניס לתמונה <a href="http://he.wikipedia.org/wiki/%D7%90%D7%9C%D7%92%D7%91%D7%A8%D7%94_%D7%9C%D7%99%D7%A0%D7%90%D7%A8%D7%99%D7%AA">אלגברה לינארית</a>.

שדה המשחק שלנו הוא מרחבים וקטוריים מעל {% equation %}\mathbb{F}_{2}{% endequation %} - השדה הסופי בן שני איברים (0 ו-1, עם כללי החיבור והכפל הרגילים, כשב"רגילים" הכוונה לכך ש-{% equation %}1+1=0{% endequation %}). "מרחב וקטורי מעל השדה הזה" הוא בסך הכל אוסף של סדרות מאורך נתון כלשהו של איברים מ-{% equation %}\mathbb{F}_{2}{% endequation %}; למשל, אם המרחב הוקטורי הוא ממימד 3, אז אברי המרחב הם {% equation %}\left(0,0,0\right),\left(0,1,0\right){% endequation %} וכן הלאה. בקיצור, טרם חידשנו משהו. כעת יש להכניס לתמונה את שאר התכונות של מרחב וקטורי: אם {% equation %}v_{1},v_{2}{% endequation %} הם איברים של המרחב הוקטורי, כך גם {% equation %}v_{1}+v_{2}{% endequation %} (כשחיבור הוא "רכיב-רכיב", כלומר {% equation %}\left(1,0,1\right)+\left(1,1,0\right)=\left(0,1,1\right){% endequation %} ). מי שמכיר אלגברה לינארית יודע שיש גם דרישה של כפל בסקלר, אבל במקרה הזה היא לא מעניינת במיוחד (למה?)

בואו נחשוב לרגע שוב על קודים באופן כללי. קוד פשוט יכול להיות קוד שלוקח מילה {% equation %}w{% endequation %} וממיר אותה במילת הקוד {% equation %}\left(w,f\left(w\right)\right){% endequation %} כש-{% equation %}f{% endequation %} היא פונקציה שמקבלת את {% equation %}w{% endequation %} ופולטת ביט בודד. אם נרצה שהקוד יהיה לינארי (ואנחנו רוצים, שכן קודים לינאריים הם קלים מאוד לניתוח), אז חיבור של שתי מילות קוד צריך לתת בעצמו מילת קוד, כלומר צריך שיתקיים ש-{% equation %}\left(w,f\left(w\right)\right)+\left(v,f\left(v\right)\right)=\left(w+v,f\left(w\right)+f\left(v\right)\right){% endequation %} יהיה בעצמו מילת קוד חוקית, כלומר שיתקיים {% equation %}f\left(w+v\right)=f\left(w\right)+f\left(v\right){% endequation %}. התכונה הזו נקראת "לינאריות של {% equation %}f{% endequation %}". נסכם - אם אנחנו רוצים לבנות את הקוד שלנו על ידי כך שנוסיף ל-{% equation %}w{% endequation %} עוד ביטים לאחריו, שמחושבים באופן מסויים מ-{% equation %}w{% endequation %}, אז כל ביט שכזה חייב להיות מחושב באמצעות פונקציה לינארית.

קוד הדמר לוקח את הגישה הזו עד הסוף - הוא אומר "בואו נוסיף אחרי {% equation %}w{% endequation %} את <strong>כל</strong> הביטים האפשריים שיכולים להתקבל מחישוב של פונקציה לינארית כלשהי". זה מייד מעלה את השאלה איך בכלל אפשר למצוא ולייצג את כל הפונקציות הלינאריות הללו. זה שוב עניין לסטודנטים לאלגברה לינארית; אבל אפשר להראות שאם {% equation %}v=\left(v_{1},v_{2},\dots,v_{n}\right){% endequation %} הוא וקטור, אז כל פונקציה לינארית היא מהצורה {% equation %}f\left(v\right)=\sum_{i=1}^{n}a_{i}v_{i}{% endequation %}, כשכל {% equation %}a_{i}{% endequation %} הוא או 0 או 1. זו הפשטה נוראית של מה שקורה במקרה הכללי יותר, של מרחב מעל שדה גדול יותר - אבל אני רוצה לא לגלוש לדיון הכללי כרגע.

כעת, הנה אבחנה מעניינת נוספת - {% equation %}a=\left(a_{1},a_{2},\dots,a_{n}\right){% endequation %} הוא בעצמו איבר המרחב הוקטורי ממימד {% equation %}n{% endequation %} מעל {% equation %}\mathbb{F}_{2}{% endequation %}, כלומר אפשר לייצג כל פונקציה לינארית מהמרחב הוקטורי הזה באמצעות <strong>איבר</strong> מהמרחב הוקטורי הזה! (גם זו תוצאה כללית בהרבה מכפי שאני מציג אותה כאן). כדי לפשט את הסימונים, מגדירים {% equation %}a\cdot v=\sum_{i=1}^{n}a_{i}v_{i}{% endequation %} - לדבר הזה קוראים "מכפלה סקלרית". מכפלה סקלרית דומה למדי לכפל "רגיל", למשל {% equation %}a\left(v_{1}+v_{2}\right)=av_{1}+av_{2}{% endequation %} (התכונה הזו תהיה חשובה בקרוב). נסו להוכיח את התכונה הזו - זה קל למדי, ומסתמך באופן חזק על התכונה הדומה עבור כפל "רגיל".

כעת, אם יש לנו מילה {% equation %}w\in\mathbb{F}_{2}^{n}{% endequation %} (כלומר, רצף של {% equation %}n{% endequation %} ספרות שהן אפס או אחד), אפשר להתעלל קצת בסימונים ולהגיד שנשתמש ב-{% equation %}w{% endequation %} גם כדי לתאר את המילה המקודדת, ולסמן בתור {% equation %}w_{a}{% endequation %} את הביט המתאים במילה המקודדת שמתקבל על ידי חישוב {% equation %}a\cdot w{% endequation %} (ובפירוט יתר: הביט שמתאים להפעלה של הפונקציה הלינארית שמיוצגת על ידי {% equation %}a{% endequation %} על המילה {% equation %}w{% endequation %}). עכשיו אפשר לשים לב לכך שההפרדה המקורית שלנו את מילת הקוד ל"קודם כל המילה המקורית, ואז התוצאה של הפעלת כמה פונקציות לינאריות עליה" היא מלאכותית למדי; גם הביטים של מילת הקוד עצמה יכולים להתקבל מהפעלה של פונקציות לינאריות. למשל, הביט הראשון של {% equation %}w{% endequation %} מתקבל על ידי כפל עם המילה {% equation %}e_{1}=\left(1,0,0,\dots,0\right){% endequation %}, הביט השני על ידי כפל עם {% equation %}e_{2}=\left(0,1,0,0,\dots,0\right){% endequation %} וכן הלאה. מעתה אמרו - הקידוד של {% equation %}w{% endequation %} הוא פשוט אוסף ההפעלות של כל הפונקציות הלינאריות האפשריות על {% equation %}w{% endequation %}. לקידוד הזה יש מחיר - אנחנו מקודדים {% equation %}n{% endequation %} ביטים באמצעות {% equation %}2^{n}{% endequation %} ביטים (כי יש {% equation %}2^{n}{% endequation %} פונקציות לינאריות אפשריות - למה?) - ניפוח אקספוננציאלי של המידע המקורי.

למה זה מועיל? כי במובן מסויים, זה לוקח את המילה {% equation %}w{% endequation %} ו"מורח" את הביטים שלה טוב-טוב. כל קומבינציה אפשרית של ביטים של {% equation %}w{% endequation %} באה לידי ביטוי איפה שהוא במילת הקוד. לכן לדגום באקראי ביט מתוך מילת הקוד בעצם שקול לדגימה אקראית של<strong> מספר כלשהו</strong> של ביטים מתוך {% equation %}w{% endequation %}; יותר במדוייק, אם אני בוחר ביט במילת קוד באקראי, אני בעצם מגריל אינדקס {% equation %}a{% endequation %} באקראי וקורא את {% equation %}w_{a}{% endequation %}; והגרלה של {% equation %}a{% endequation %} באופן אקראי (בהתפלגות אחידה) פירושו שכל קוארדינטה של {% equation %}a{% endequation %} מוגרלת בהסתברות {% equation %}\frac{1}{2}{% endequation %} ל-0 ו-{% equation %}\frac{1}{2}{% endequation %} ל-1. כלומר, בדגימה אקראית של ביט ממילת הקוד אנחנו מגרילים כל ביט מ-{% equation %}w{% endequation %} בהסתברות חצי. האבחנה הזו היא המפתח לכך שקוד הדמר ניתן לבדיקה מקומית.

בואו ננסח במדוייק כעת מה מטרתו של בודק מקומי עבור הקוד. הדרישה היא שבהינתן מילת קוד חוקית, הבודק יגיד "כן", ובהינתן מילה שאינה מילת קוד, הבודק יגיד "לא" בהסתברות סבירה, שתלויה במרחק המילה ממילת קוד חוקית. נאמר שהמילה {% equation %}w{% endequation %} רחוקה עד כדי {% equation %}\delta{% endequation %} ממילת קוד חוקית, כש-{% equation %}0&lt;\delta\le1{% endequation %}, אם צריך לשנות {% equation %}\delta{% endequation %} מהכניסות של {% equation %}w{% endequation %} כדי לקבל מילת קוד חוקית (למשל, אם {% equation %}\delta=\frac{1}{4}{% endequation %} זה אומר שצריך לשנות רבע מהכניסות של {% equation %}w{% endequation %}). לאלו מכם שרוצים פורמליזם, ההגדרה המדוייקת היא {% equation %}\delta=\min_{w^{\prime}\in C}\frac{d\left(w,w^{\prime}\right)}{\left|w\right|}{% endequation %}. זהו בעצם המרחק במשמעותו המקורית כשהוא מנורמל באופן שמתחשב באורך המילים - "המרחק היחסי" שבין המילים. מכניסים אותו לתמונה כי הוא מפשט את הניסוחים הטכניים.

אם כן, מה שאראה הוא בודק מקומי שעל מילה שאיננה מילת קוד חוקית, מזהה זאת בהסתברות של {% equation %}\frac{\delta}{2}{% endequation %} לפחות, אך לא יותר מ-{% equation %}\frac{2}{9}{% endequation %}. זאת אומרת שאם {% equation %}\delta&lt;\frac{4}{9}{% endequation %}, אז ההסתברות שתתגלה שגיאה היא {% equation %}\frac{\delta}{2}{% endequation %} (ולכן עבור {% equation %}\delta{% endequation %} קטן היא לא גדולה; אבל היא לינארית ב-{% equation %}\delta{% endequation %}, וזה טוב), ועבור ערכים גדולים יותר יש הסתברות קבועה של {% equation %}\frac{2}{9}{% endequation %} לגלות שגיאה ולכן על ידי הפעלות נשנות של הבודק אפשר "לנפח" את ההסתברות לגילוי שגיאה עד שתתקרב כרצוננו ל-1. ה"מחיר" יהיה קריאה של בדיוק שלושה ביטים מהקלט; כמובן שה"ניפוח" מצריך קריאה של עוד ביטים, אבל עדיין מספר קבוע שאינו תלוי ב-{% equation %}n{% endequation %}. כאן אנחנו מתחילים להרגיש את ה-PCP שהזכרתי בפוסט הקודם; גם שם הרעיון הוא קריאה של מספר קבוע של ביטים.

ואיך הבודק עובד? באופן כמעט מגוחך. הוא בוחר באקראי וקטורים {% equation %}a,b{% endequation %} וקורא את {% equation %}w_{a},w_{b}{% endequation %}. כזכור, הערכים הללו הם {% equation %}a\cdot w,b\cdot w{% endequation %}, ועל פי כללי הכפל נובע ש-{% equation %}aw+bw=\left(a+b\right)w{% endequation %}, מה שמוביל אותנו בקלות לביט הנוסף שיש לדגום - הוא קורא את {% equation %}w_{a+b}{% endequation %} ובודק האם {% equation %}w_{a+b}=w_{a}+w_{b}{% endequation %}. אם כן - הוא מקבל (או מתחיל סיבוב בדיקה חדש); אם לא, הוא דוחה. כל כך פשוט.

טוב, אבל למה זה עובד? התשובה הקצרה היא "זה טכני". התשובה הארוכה היא "זה טכני, אבל אראה את זה בכל זאת ובתקווה לא אאבד יותר מדי אנשים, כי זו אחת מההוכחות האהובות עלי". אני מקווה שברור מדוע אם {% equation %}w{% endequation %} היא מילת קוד חוקית הבדיקה עובדת - נימקתי זאת לעיל. השאלה היא מה קורה אם {% equation %}w{% endequation %} אינה מילת קוד חוקית.

הבה ונסמן את ההסתברות שהבודק ידחה את המילה {% equation %}w{% endequation %} ב-{% equation %}\varepsilon{% endequation %}. אם {% equation %}\varepsilon\ge\frac{2}{9}{% endequation %}, "ניצחנו" - הראנו שהבודק עובד טוב על {% equation %}w{% endequation %}. לכן ההנחה היא ש-{% equation %}\varepsilon&lt;\frac{2}{9}{% endequation %}. מה שרוצים להראות הוא שמכך נובע שהמרחק היחסי של {% equation %}w{% endequation %} ממילת הקוד החוקית הקרובה ביותר (שהוא, כזכור, {% equation %}\delta{% endequation %}) לא עולה על {% equation %}2\varepsilon{% endequation %} (כלומר - {% equation %}\delta\le2\varepsilon{% endequation %}, או {% equation %}\varepsilon\ge\frac{\delta}{2}{% endequation %}). בקיצור - רוצים למצוא איכשהו מילת קוד חוקית שקרובה יחסית ל-{% equation %}w{% endequation %}. אה, אבל זה בדיוק מה שקודים לתיקון שגיאות עושים - בהינתן מילה "מקולקלת", מראים כיצד לתקן אותה.

כאן מגיע הרעיון המרכזי בהוכחה, שהוא מקסים. כיצד תיבנה אותה מילת קוד, שאסמן בתור {% equation %}v{% endequation %}? באופן הבא. הכניסה {% equation %}v_{a}{% endequation %} במילה תוגדר בתור "הצבעת הרוב" של {% equation %}w{% endequation %}. למה הכוונה? אנחנו יודעים ש<strong>אם</strong> {% equation %}w{% endequation %} הייתה מילה חוקית, אז היה מתקיים {% equation %}w_{a}+w_{b}=w_{a+b}{% endequation %} לכל {% equation %}b{% endequation %} אפשרי, או בניסוח אחר - {% equation %}w_{a}=w_{a+b}-w_{b}{% endequation %}. לרוע המזל {% equation %}w{% endequation %} איננה מילת קוד חוקית ולכן זה לא מתקיים עבורה תמיד - כלומר, ייתכן ש-{% equation %}w_{a+b}-w_{b}{% endequation %} לא תמיד מחזיר את אותו ערך, כאשר משנים את הערכים ש-{% equation %}b{% endequation %} מקבל. עם זאת, הערך שמופיע מספר רב יותר של פעמים הוא כנראה הערך ה"נכון" עבור {% equation %}w{% endequation %}. אם כן, התיקון של {% equation %}w{% endequation %} יהיה {% equation %}v{% endequation %} כך ש-{% equation %}v_{a}=\mbox{majority}_{b}\left\{ w_{a+b}-w_{b}\right\} {% endequation %}, כאשר {% equation %}\mbox{majority}{% endequation %} היא פונקציה שמחזירה את האיבר השכיח יותר ב"קבוצה" שהיא מקבלת ("קבוצה" במרכאות כי בדרך כלל בקבוצות כל איבר נספר פעם אחת, וכאן למספר המופעים יש חשיבות מכרעת).

צריך כעת להראות שני דברים: ראשית ש-{% equation %}v{% endequation %} היא בכלל מילת קוד חוקית; שנית, שהיא קרובה ל-{% equation %}w{% endequation %} עד כדי {% equation %}2\varepsilon{% endequation %} - פורמלית נסמן זאת {% equation %}d\left(w,v\right)&lt;\varepsilon{% endequation %}. נתחיל דווקא מהתוצאה השניה. האינטואיציה כאן אינה קשה - אם {% equation %}w_{a}\ne v_{a}{% endequation %}, אז אם הבודק המקומי יבחר את {% equation %}a{% endequation %} כאחת משתי הקוארדינטות שהוא בודק, יהיה לו סיכוי של לפחות חצי לעלות על שגיאה, שהרי {% equation %}w_{a}{% endequation %} <strong>אינו</strong> מתאים לכלל הצבעת הרוב, ולכן עבור רוב הערכים של {% equation %}b{% endequation %} שהבודק יגריל, יתקיים ש-{% equation %}w_{a}\ne w_{a+b}-w_{b}{% endequation %} (כלומר, {% equation %}w_{a}+w_{b}\ne w_{a+b}{% endequation %}). כעת, שימו לב שהסיכוי של הבודק ליפול על {% equation %}a{% endequation %} שמקיים {% equation %}w_{a}\ne v_{a}{% endequation %} הוא בדיוק המרחק היחסי שלהם; ואם הבודק כבר נפל על {% equation %}a{% endequation %} שכזה, ההסתברות שלו ליפול על {% equation %}b{% endequation %} שיכשיל את {% equation %}w{% endequation %} היא לפחות חצי; ולכן ההסתברות של הבודק לדחות את {% equation %}w{% endequation %} היא לפחות {% equation %}\frac{1}{2}d\left(w,v\right){% endequation %}; אבל סימנו ב-{% equation %}\varepsilon{% endequation %} את ההסתברות של הבודק לדחות את {% equation %}w{% endequation %}, כלומר {% equation %}\frac{1}{2}d\left(w,v\right)\le\varepsilon{% endequation %}, כלומר {% equation %}\delta\le d\left(w,v\right)\le2\varepsilon{% endequation %}.

אם כן, נותר אתגר בודד - להראות ש-{% equation %}v{% endequation %} היא מילת קוד חוקית. כפי שניתן לנחש, זה המקום שבו {% equation %}\frac{2}{9}{% endequation %} המסתורי יבוא לידי ביטוי. ברשותכם, ארשה לעצמי להמשיך להיות טכני ולהציג את ההוכחה במלואה. ראשית כל צריך להבהיר לעצמנו כיצד ניתן להוכיח שמילה כלשהי {% equation %}v{% endequation %} היא מילת קוד חוקית. דרך אחת היא להראות איך מילה בת {% equation %}n{% endequation %} ביטים ממופה ל-{% equation %}v{% endequation %} על ידי הקוד; אבל זה די מסורבל. הרבה יותר נחמד להשתמש בקריטריון הבדיקה שכבר הצגנו, ומסתבר שזה אכן מספיק: אם {% equation %}v_{a}+v_{b}=v_{a+b}{% endequation %} לכל {% equation %}a,b{% endequation %}, אז {% equation %}v{% endequation %} היא מילת קוד חוקית. האבחנה הזו טבעית למדי - הרי כפי שראינו, גם הביטים של "המילה המקורית" שממנה התקבל הקוד מקודדים בתוך {% equation %}v{% endequation %} (הביט ה-{% equation %}i{% endequation %} מקודד כ-{% equation %}v_{e_{i}}{% endequation %}) ומתכונת החיבוריות הזו נובע שלכל {% equation %}a=\sum\alpha_{i}e_{i}{% endequation %} מתקיים {% equation %}v_{a}=v_{\sum\alpha_{i}e_{i}}=\sum\alpha_{i}v_{e_{i}}{% endequation %}, כנדרש.

אם כן, מספיק לקחת {% equation %}a,b{% endequation %} שרירותיים ולהראות שעבורם מתקיים {% equation %}v_{a}+v_{b}=v_{a+b}{% endequation %}. באופן די מפתיע, ההוכחה היא כמעט מיידית אם רק נראה עוד משהו אחד - ש"הכרעת הרוב" שמגדירה את {% equation %}v_{a}{% endequation %} היא לא סתם הכרעת רוב, אלא הכרעת רוב מוחץ: שלפחות {% equation %}\frac{2}{3}{% endequation %} מה"הצבעות" {% equation %}w_{a+b}-w_{a}{% endequation %} נתנו את הערך של {% equation %}v_{a}{% endequation %} ולא את הערך השני האפשרי. קודם כל אסביר מדוע זה מסיים את העניין ואז אוכיח זאת.

הטריק הוא להשתמש בשיטה ההסתברותית - להוכיח שמשהו קיים על ידי כך שמראים שההסתברות למציאתו במרחב החיפוש שלנו גדולה מאפס. מה שאנו מחפשים הוא מילה {% equation %}c{% endequation %} שתקיים את שלוש התכונות הבאות <strong>בו זמנית</strong>:
<ol>
	<li> {% equation %}v_{a}=w_{c}-w_{a+c}{% endequation %}</li>
	<li> {% equation %}v_{b}=w_{b+c}-w_{c}{% endequation %}</li>
	<li> {% equation %}v_{a+b}=w_{b+c}-w_{a+c}{% endequation %}</li>
</ol>
אם מצאנו {% equation %}c{% endequation %} כזה, סיימנו. למה? שכן אז {% equation %}v_{a}+v_{b} = \left(w_{c}-w_{a+c}\right)+\left(w_{b+c}-w_{c}\right)=w_{b+c}-w_{a+c}=v_{a+b}{% endequation %} כנדרש. אז למה קיים {% equation %}c{% endequation %} כזה? ובכן, בואו נביט לרגע על המשוואה הראשונה, {% equation %}v_{a}=w_{c}-w_{a+c}{% endequation %}. מכיוון שפעולת החיבור מתבצעת מעל {% equation %}\mathbb{F}_{2}{% endequation %} (כלומר, {% equation %}1+1=0{% endequation %}, או במילים אחרות {% equation %}1=-1{% endequation %}), זו בדיוק אותה משוואה כמו {% equation %}v_{a}=w_{a+c}-w_{c}{% endequation %}

מה ההסתברות, אם מגרילים {% equation %}c{% endequation %}, שהיא לא מתקיימת? בדיוק ההסתברות ש-{% equation %}c{% endequation %} יהיה אחד מקולות המיעוט בהצבעה שקבעה את {% equation %}v_{a}{% endequation %}. מכיוון שאמרנו שהרוב בהצבעה היה לפחות {% equation %}\frac{2}{3}{% endequation %}, נובע מכך שההסתברות ש-{% equation %}c{% endequation %} יהיה "מקלקל" שכזה היא לכל היותר {% equation %}\frac{1}{3}{% endequation %}. באופן דומה גם עבור שני התנאים האחרים אפשר להראות שההסתברות לכך ש-{% equation %}c{% endequation %} לא יקיים אותם היא לכל היותר {% equation %}\frac{1}{3}{% endequation %}. כעת, <a href="http://en.wikipedia.org/wiki/Boole%27s_inequality">חסם טריוויאלי</a> בהסתברות (שמכונה Union bound) אומר כי אם יש לנו קבוצת מאורעות, אז ההסתברות שלפחות אחד מהם יתרחש היא לכל היותר סכום ההסתברויות של כולם. כאן יש לנו שלושה מאורעות שההסתברות של כל אחד מהם קטנה מ-{% equation %}\frac{1}{3}{% endequation %}, ולכן ההסתברות שלפחות אחד מהם יקרה קטנה מ-{% equation %}1{% endequation %} - כלומר, קיים {% equation %}c{% endequation %} שלא מקיים אף אחד מהמאורעות. מכיוון שהמאורעות היו "1 מתקלקל", "2 מתקלקל" ו-"3 מתקלקל", קיבלנו שיש {% equation %}c{% endequation %} שעבורו אף אחד משלושת התנאים אינו מתקלקל - כלומר, סיימנו.

נותר רק להראות שאכן מתקיימת "הכרעת רוב מוחץ", כלומר שעבור לפחות {% equation %}\frac{2}{3}{% endequation %} מה-{% equation %}b{% endequation %}-ים האפשריים מתקיים {% equation %}v_{a}=w_{a+b}-w_{b}{% endequation %}.

הבה ונסמן את גודל הרוב ב-{% equation %}p{% endequation %}, כלומר {% equation %}\mbox{Pr}\left[v_{a}=w_{a+b}-w_{b}\right]=p{% endequation %} ({% equation %}\mbox{Pr}{% endequation %} הוא סימון סטנדרטי להסתברות של המאורע שבסוגריים; ההסתברות נלקחת על פני הבחירות האקראיות של {% equation %}b{% endequation %}). כעת נשדרג את המשחק - נניח שאנחנו בוחרים באקראי <strong>שני</strong> "מצביעים", {% equation %}b,c{% endequation %} ושואלים אותם לדעתם; מה ההסתברות שהם יגידו את אותו הדבר? כלומר, מהו {% equation %}\mbox{Pr}\left[w_{a+b}-w_{b}=w_{a+c}-w_{c}\right]{% endequation %}? ובכן, אחד משניים: או ששניהם הצביעו לערך שהרוב בחרו, בהסתברות {% equation %}p{% endequation %} כל אחד ולכן {% equation %}p^{2}{% endequation %} לשניהם יחד; או ששניהם הצביעו עבור הערך השני, בהסתברות {% equation %}\left(1-p\right){% endequation %} כל אחד ולכן {% equation %}\left(1-p\right)^{2}{% endequation %} לשניהם יחד. סה"כ ההסתברות שהם מסכימים היא {% equation %}p^{2}+\left(1-p\right)^{2}{% endequation %}. אם נצליח למצוא חסם תחתון כלשהו על ההסתברות הזו, נוכל לחלץ מהמשוואה (ומכך ש-{% equation %}p\ge\frac{1}{2}{% endequation %}) חסם תחתון על {% equation %}p{% endequation %}. כאן גם מתחילה ה"הנדסה לאחור" שלבסוף תניב את ה-{% equation %}\frac{2}{9}{% endequation %} הידוע לשמצה - אנחנו מחפשים חסם תחתון על {% equation %}p^{2}+\left(1-p\right)^{2}{% endequation %} שיגרור {% equation %}p&gt;\frac{2}{3}{% endequation %}; קצת משחק בפרמטרים ופתרון משוואה ריבועית יראה כי החסם התחתון הזה הוא {% equation %}\frac{5}{9}{% endequation %} (נסו זאת בבית!).

נסכם: עלינו להראות כי {% equation %}\mbox{Pr}\left[w_{a+b}-w_{b}=w_{a+c}-w_{c}\right]&gt;\frac{5}{9}{% endequation %}. הטכניקה דומה לטכניקה שכבר השתמשתי בה. ראשית נשים לב לכך ש-{% equation %}\mbox{Pr}\left[w_{a+b}-w_{b}=w_{a+c}-w_{c}\right] = \mbox{Pr}\left[w_{a+b}+w_{c}=w_{a+c}+w_{b}\right]{% endequation %}

כעת שני האגפים הם "מאוזנים" במידת מה - בכולם מופיעים גם {% equation %}a{% endequation %}, גם {% equation %}b{% endequation %} וגם {% equation %}c{% endequation %}. יותר מכך - אם {% equation %}w{% endequation %} הייתה מילת קוד חוקית, שני האגפים היו שווים ל-{% equation %}w_{a+b+c}{% endequation %}. לכן ההסתברות של המאורע שלמטה היא בעצם ההסתברות שלא חל "קלקול" בחישוב של {% equation %}w_{a+b+c}{% endequation %} לא באמצעות {% equation %}w_{a+b}+w_{c}{% endequation %}, וגם לא באמצעות {% equation %}w_{a+c}+w_{b}{% endequation %}. לכן שוב נשאל את עצמנו - מה ההסתברות שכן חל קלקול באחד משני המקרים הללו?

אם כן, מה ההסתברות ש-{% equation %}w_{a+b}+w_{c}\ne w_{a+b+c}{% endequation %}? זה טיפה מבלבל מכיוון שהן {% equation %}b{% endequation %} והן {% equation %}c{% endequation %} נבחרו באקראי ואילו {% equation %}a{% endequation %} נקבע מראש, אבל אפשר לפשט קצת את העניינים: אם {% equation %}b{% endequation %} נבחר באקראי ובהתפלגות אחידה מבין כל הערכים האפשריים, ו-{% equation %}a{% endequation %} קבוע, אז גם {% equation %}a+b{% endequation %} מוגרל בהתפלגות אחידה מבין כל הערכים האפשריים (למה?). לכן אפשר לסמן לצורך פשטות {% equation %}a^{\prime}=a+b{% endequation %} ולשאול את עצמנו את השאלה הפשוטה יותר: מהי ההסתברות ש-{% equation %}w_{a^{\prime}}+w_{c}\ne w_{a^{\prime}+c}{% endequation %} כאשר הן {% equation %}a^{\prime}{% endequation %} והן {% equation %}c{% endequation %} נבחרים באקראי? ואת התשובה לשאלה הזו אנחנו יודעים: ההסתברות הזו חסומה על ידי ההסתברות שהבודק ידחה את {% equation %}w{% endequation %} - שהרי זה בדיוק מה שהבודק עושה - מגריל שני אינדקסים ומבצע את בדיקת השוויון שלעיל.

כזכור, בתחילת הדיון הזה הנחנו שהסתברות הדחייה של הבודק היא נמוכה יחסית - נמוכה מ-{% equation %}\frac{2}{9}{% endequation %} (כי במקרה שהיא גבוהה יותר אין מה להראות). מכאן שההסתברות ש-{% equation %}w_{a+b}+w_{c}\ne w_{a+b+c}{% endequation %} קטנה מ-{% equation %}\frac{2}{9}{% endequation %}, וגם ההסתברות ש-{% equation %}w_{a+c}+w_{b}\ne w_{a+b+c}{% endequation %} קטנה מ-{% equation %}\frac{2}{9}{% endequation %}, ולכן על פי ה-Union bound נקבל שההסתברות שאף אחד משני מאורעות אלו אינו מתרחש, ולכן {% equation %}w_{a+b}+w_{c}=w_{a+c}+w_{b}{% endequation %}, היא לפחות {% equation %}\frac{5}{9}{% endequation %}, כמו שרצינו. זה מסיים, סוף כל סוף, את ההוכחה כולה.

תרגיל בית למי שרוצה לוודא שהוא אכן הבין את כל החישובים הקטנים והקטנוניים האחרונים, ושה-{% equation %}\frac{2}{9}{% endequation %} שהופיע בהתחלה לא נפל משמיים אלא אפשר להגיע אליו בדרך טבעית: נניח שהיינו רוצים להראות ש-{% equation %}p&gt;\frac{3}{4}{% endequation %} ולא סתם {% equation %}p&gt;\frac{2}{3}{% endequation %}; מהו החסם החדש על {% equation %}\varepsilon{% endequation %} שהיה עלינו לדרוש?
