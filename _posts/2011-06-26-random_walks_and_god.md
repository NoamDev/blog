---
id: 1205
title: "מהמרים כושלים, שיכורים בביוב, ואלוהים"
date: 2011-06-26 15:08:11
layout: post
categories: 
  - הסתברות
  - מתמטיקה בראי התקשורת
tags: 
  - התרוששות המהמר
  - נוסחאות נסיגה
  - שרשראות מרקוב
  - תוצאות לא אינטואיטיביות
---
הטריגר לפוסט הזה הוא המאמר <a href="http://www.ynet.co.il/articles/0,7340,L-4086098,00.html">הבא</a> ב-Ynet שמנסה לשכנע אותנו שיש אלוהים <strong>בגלל</strong> שיש אבולוציה. אבל נחכה עם החלק הלא מתמטי לסוף - הסיבה שאני כותב את הפוסט הזה היא שהוא מזכיר שם תוצאה מתמטית קטנה ונחמדה שהיא פתח לתחום מעניין או שניים ומזמן רציתי לכתוב עליה. התוצאה הזו נקראת לעתים "התרוששות המהמר" (Gambler's Ruin), אף כי לא תמיד קוראים לה כך, ולפעמים קוראים בשם "התרוששות המהמר" גם לדברים אחרים.

במה העניין? נניח שאתם נכנסים לקזינו הוגן. "הוגן", במובן זה שהסיכוי שלכם להרוויח בסיבוב כלשהו זהה לסיכוי שלכם להפסיד בו. ואתם מהמרים ומהמרים כאילו אין מחר. ונניח גם שהקזינו לא יכול להתרושש אף פעם - לא משנה איזה סכום תרוויחו, יש לו כיסוי לכך. המסקנה המתמטית: אתם הולכים להתרושש, ובגדול. מובטח לכם, בהסתברות של 100 אחוז, שאם תהמרו לעד, בסוף תפסידו הכל.

אותי התוצאה הזו הפתיעה למדי, אבל אני משער שלחלקכם היא מרגישה מובנת מאליה - מי שמהמר עד אינסוף, ברור שיפסיד הכל בסופו של דבר, לא? אלא שגם האינטואיציה הזו לא נכונה, כי אם הקזינו מוטה לטובתכם, ואפילו אם זה אומר רק שיש לכם הסתברות של 51 לזכות בכל סיבוב, הרי שיש לכם סיכוי כלשהו להמשיך עם משחק ההימורים לנצח מבלי להפסיד הכל. אבל בקזינו הוגן (וגם, כצפוי, קזינו שמוטה לרעתכם, כפי שקורה במציאות) מובטח לכם להפסיד.

נשאלת השאלה, איך מוכיחים את זה מתמטית, ואפילו איך ממדלים את זה. לצורך פשטות נניח שבכל הימור אתם מרוויחים או מפסידים שקל, בהסתברות {::nomarkdown}\( p\){:/nomarkdown} (כאשר הקזינו הוגן, {::nomarkdown}\( p=\frac{1}{2}\){:/nomarkdown}). אז אפשר למדל את הסיטואציה הזו בתור <strong>הילוך אקראי</strong> - הילוך שכזה הוא סדרה של "מצבים", שמכל אחד מהם עוברים באופן אקראי למצב הבא בתור. כאן כל מצב מאופיין על ידי כמות הכסף שלנו, כלומר יש מצב לכל מספר טבעי (מצב מס' 0 הוא זה שבו יש לנו אפס שקלים, מצב מס' 1 הוא זה שבו יש לנו שקל אחד וכדומה). אם אנחנו במצב {::nomarkdown}\( n\){:/nomarkdown}, אז בהסתברות חצי אנו עוברים למצב {::nomarkdown}\( n-1\){:/nomarkdown} ובהסתברות חצי למצב {::nomarkdown}\( n+1\){:/nomarkdown}. המצב החריג היחיד הוא מצב 0, שממנו אין שום יציאה, אבל ממילא לא נתעניין בגורל הילוך מקרי מרגע שהוא כבר הגיע למצב 0, אלא רק בהסתברות שלו להגיע למצב 0 אם הוא התחיל במקום אחר.

כרגיל, אנחנו רק מגרדים פה את קצה הקרחון של תורה עמוקה יותר. באופן כללי סיטואציה כזו, שבה אנחנו נמצאים בתוך אחד מבין קבוצה של "מצבים", ובוחרים באקראי את המצב הבא על פי הגרלה שתלויה אך ורק במצב הנוכחי שלנו (להבדיל מהגרלה שתלויה גם ב"היסטוריה" שלנו - למשל, המצב שבו היינו מייד לפני המצב הנוכחי) - נקראת <strong>שרשרת מרקוב</strong> (ליתר דיוק - שרשרת מרקוב בדידה בזמן בדיד, אבל נעזוב את זה). לשרשראות מרקוב תורה יפה וכללית משלהן, שלא אציג כאן. אציג רק את המשפט שרלוונטי לענייננו, זה שנוגע ל"זמני פגיעה" במצבים.

מרקוב אומר - קחו שרשרת. וכעת קחו קבוצת מצבים {::nomarkdown}\( A\){:/nomarkdown}, שאתם רוצים לדעת מה ההסתברות "לפגוע" בהם אם התחלתם ממצב {::nomarkdown}\( i\){:/nomarkdown} כלשהו. נסמן ב-{::nomarkdown}\( h_{i}^{A}\){:/nomarkdown} את ההסתברות הזו (לפגוע ב-{::nomarkdown}\( A\){:/nomarkdown} מתישהו אם התחלתם מ-{::nomarkdown}\( i\){:/nomarkdown}). ברור ש-{::nomarkdown}\( h_{i}^{A}=1\){:/nomarkdown} אם {::nomarkdown}\( i\in A\){:/nomarkdown}; ולא קשה לראות שעבור {::nomarkdown}\( i\notin A\){:/nomarkdown} צריך להתקיים {::nomarkdown}\( h_{i}^{A}=\sum_{j}p_{ij}h_{j}^{A}\){:/nomarkdown} כאשר {::nomarkdown}\( p_{ij}\){:/nomarkdown} הוא ההסתברות לעבור מהמצב {::nomarkdown}\( i\){:/nomarkdown} למצב {::nomarkdown}\( j\){:/nomarkdown} (הסתברות שיכולה להיות גם 0).

את ה"לא קשה לראות" הזה כדאי להסביר עוד קצת. בפוסט הקודם, שעסק בבעיית המזכירה, השתמשתי בנוסחת ההסתברות השלמה; גם כאן הנוסחה שלעיל היא פשוט שימוש בנוסחת ההסתברות השלמה, כאשר האירועים שמחלקים את המרחב שלנו הם מהצורה "מ-{::nomarkdown}\( i\){:/nomarkdown} אני אעבור אל {::nomarkdown}\( j\){:/nomarkdown}". אם כבר ידוע שעברת ל-{::nomarkdown}\( j\){:/nomarkdown}, אז ההסתברות שתפגע ב-{::nomarkdown}\( A\){:/nomarkdown} היא בדיוק ההסתברות {::nomarkdown}\( h_{j}^{A}\){:/nomarkdown} - ההסתברות שתפגע ב-{::nomarkdown}\( A\){:/nomarkdown} אם אתה מתחיל מ-{::nomarkdown}\( j\){:/nomarkdown}. כאן אני משתמש בצורה חזקה בכך שלשרשרת מרקוב אין זכרון, ולכן ההסתברות שלך לפגוע ב-{::nomarkdown}\( A\){:/nomarkdown} אם <strong>עברת</strong> אל {::nomarkdown}\( j\){:/nomarkdown} זהה להסתברות שלך לפגוע ב-{::nomarkdown}\( A\){:/nomarkdown} אם <strong>התחלת</strong> ב-{::nomarkdown}\( j\){:/nomarkdown} - אין חשיבות להיסטוריה.

יופי. כעת בא מרקוב ואומר שהפתרון האי שלילי המינימלי למערכת המשוואות שנתנו עבור ה-{::nomarkdown}\( h_{i}^{A}\){:/nomarkdown} הוא גם מה שקורה בפועל בשרשרת. זו לא טענה טריוויאלית לחלוטין וההוכחה שלה טיפה טכנית ולכן לא אציג אותה כרגע, אבל היא מן הסתם נמצאת בכל ספר לימוד בנושא (אני מקווה...). בואו נעבור ליישם אותה על המקרה שלנו.

במקרה שלנו, הנוסחה יוצאת פשוטה למדי. ראשית, {::nomarkdown}\( A\){:/nomarkdown} תכיל רק את מצב מס' 0, ונרשום {::nomarkdown}\( h_{i}\){:/nomarkdown} בלי ה-{::nomarkdown}\( A\){:/nomarkdown} למעלה כי זה סתם מיותר. כעת, מערכת המשוואות שלנו כוללת רק משוואות מהצורה הבאה: {::nomarkdown}\( h_{i}=ph_{i+1}+qh_{i-1}\){:/nomarkdown}, כאשר {::nomarkdown}\( p\){:/nomarkdown} הוא ההסתברות לנצח בהימור של הסיבוב הנוכחי ולהרוויח שקל אחד ו-{::nomarkdown}\( q\){:/nomarkdown} היא ההסתברות להפסיד. מכיוון שאו שמנצחים או שמפסידים, {::nomarkdown}\( p+q=1\){:/nomarkdown}. כמו כן, {::nomarkdown}\( h_{0}=1\){:/nomarkdown}, כמובן.

אם נרשום את המשוואה שלמעלה בצורה טיפה שונה, נקבל {::nomarkdown}\( ph_{i+1}=h_{i}-qh_{i-1}\){:/nomarkdown}. זוהי <strong>נוסחת נסיגה</strong>, ואנחנו יודעים לפתור נוסחאות כאלו בקלות עוד <a href="http://www.gadial.net/?p=199">מימי פיבונאצ'י העליזים בבלוג</a>. בואו נזכיר את הרעיון בקצרה - בהינתן משוואת נסיגה {::nomarkdown}\( a_{n}=\alpha a_{n-1}+\beta a_{n-2}\){:/nomarkdown} אנחנו "מנחשים" (ניחוש מושכל, כמובן) שיש למשוואה פתרון מהצורה {::nomarkdown}\( h_{n}=\lambda^{n}\){:/nomarkdown}, מציבים את הפתרון בנוסחת הנסיגה, מקבלים {::nomarkdown}\( \lambda^{n}=\alpha\lambda^{n-1}+\beta\lambda^{n-2}\){:/nomarkdown}, מחלקים ב-{::nomarkdown}\( \lambda^{n-2}\){:/nomarkdown} (תוך הנחה ש-{::nomarkdown}\( \lambda\ne0\){:/nomarkdown} אחרת הפתרון "לא מעניין") ומקבלים משוואה ריבועית {::nomarkdown}\( \lambda^{2}=\alpha\lambda+\beta\){:/nomarkdown}. למשוואה הזו אנחנו מוצאים את שני הפתרונות {::nomarkdown}\( \lambda_{1},\lambda_{2}\){:/nomarkdown} שנותנים לנו שני פתרונות לנוסחת הנסיגה, ואז קורה קסם - מתברר שכל פתרון אחר למשוואה ניתן לכתיבה כצירוף לינארי של שניהם, כלומר {::nomarkdown}\( a_{n}=A\lambda_{1}^{n}+B\lambda_{2}^{n}\){:/nomarkdown} הוא פתרון כללי לנוסחת הנסיגה. כל שנותר לעשות הוא להציב את תנאי ההתחלה של המשוואה כדי לגלות את ערכם של הקבועים {::nomarkdown}\( A,B\){:/nomarkdown}.

אז זו התיאוריה הכללית, והיא עבדה נפלא במקרה של פיבונאצ'י. במקרה שלנו תכף נראה שנצטרך להיות טיפה יותר מתוחכמים.

ראשית, מהי המשוואה הריבועית שמתקבלת מהנוסחה {::nomarkdown}\( ph_{i+1}=h_{i}-qh_{i-1}\){:/nomarkdown}? אחרי העברת אגפים והצבה וחלוקה וכל זה נקבל {::nomarkdown}\( p\lambda^{2}-\lambda+q=0\){:/nomarkdown}. פתרון על פי נוסחת השורשים מניב {::nomarkdown}\( \lambda_{1,2}=\frac{1\pm\sqrt{1-4pq}}{2p}\){:/nomarkdown} שלא נראה יפה במיוחד. מה עושים?

ובכן, ידוע לנו ש-{::nomarkdown}\( p+q=1\){:/nomarkdown}, בואו נשתמש בזה! אפשר להעלים את {::nomarkdown}\( q\){:/nomarkdown} לחלוטין מהתמונה על ידי ההצבה {::nomarkdown}\( q=1-p\){:/nomarkdown}, ואז לקבל מתחת לשורש את הביטוי {::nomarkdown}\( 1-4p\left(1-p\right)\){:/nomarkdown}, ששווה ל-{::nomarkdown}\( 4p^{2}-4p+1\){:/nomarkdown}, ששווה ל-{::nomarkdown}\( \left(2p-1\right)^{2}\){:/nomarkdown}. כאן זה עדיין תעלול של בית ספר תיכון. אחרי הוצאת השורש נקבל {::nomarkdown}\( \lambda_{1,2}=\frac{1\pm\left(2p-1\right)}{2p}\){:/nomarkdown}, ולכן שני הפתרונות הם 1 ו-{::nomarkdown}\( \frac{1}{p}-1\){:/nomarkdown}. את השני אפשר לכתוב בצורה קצת יותר יפה לטעמנו: {::nomarkdown}\( \frac{1}{p}-1=\frac{1-p}{p}=\frac{q}{p}\){:/nomarkdown}. רואים? לא שכחנו את {::nomarkdown}\( q\){:/nomarkdown}; כשהוא עוזר לנו להבין דברים, מחזירים אותו.

אולי שמתם לב כבר למשהו מוזר שקורה כאן - אם {::nomarkdown}\( p=q\){:/nomarkdown} (וזה, כמובן, המקרה שהכי מעניין אותנו - קזינו הוגן), אז לא קיבלנו שני פתרונות אלא את אותו הפתרון - {::nomarkdown}\( \lambda=1\){:/nomarkdown} - פעמיים. זה תוקע אותנו לגמרי - כל הרעיון היה שאנחנו מצליחים למצוא שני פתרונות בלתי תלויים שמהם אפשר להרכיב כל פתרון. אז מה עושים במקרה הזה? סבלנות, בואו נטפל קודם כל במקרה שבו אנחנו כבר יודעים איך לטפל.

אם כן, אם {::nomarkdown}\( p\ne q\){:/nomarkdown} קיבלנו שהפתרון הכללי לנוסחת הנסיגה הוא מהצורה {::nomarkdown}\( h_{n}=A\cdot1^{n}+B\cdot\left(\frac{q}{p}\right)^{n}=A+B\left(\frac{q}{p}\right)^{n}\){:/nomarkdown}. יש לנו תנאי התחלה אחד: {::nomarkdown}\( h_{0}=1\){:/nomarkdown}; אבל רגע, יש לפתרון שתי דרגות חופש (גם הערך של {::nomarkdown}\( A\){:/nomarkdown} וגם הערך של {::nomarkdown}\( B\){:/nomarkdown} לא ידועים), אז צריך שני תנאי התחלה! כלומר, אין לנו בכלל דרך למצוא פתרון יחיד למשוואה. מה עושים? נראה ששוב התיאוריה מאכזבת אותנו.

כאן מרקוב עוזר לנו. כזכור, מרקוב אומר שהפתרון ה"נכון" לנוסחת הנסיגה - זה שבאמת מתאר את מה שקורה בשרשרת, הוא <strong>הפתרון האי שלילי המינימלי</strong>. "מינימלי" כאן פירושו שאם {::nomarkdown}\( k_{n}\){:/nomarkdown} הוא פתרון אי שלילי אחר לנוסחת הנסיגה, אז {::nomarkdown}\( h_{n}\le k_{n}\){:/nomarkdown} לכל {::nomarkdown}\( n\){:/nomarkdown}. בנוסף, יש לנו עוד יתרון - אנחנו יודעים שהפתרונות של הנוסחה מייצגים <strong>הסתברות</strong> ולכן {::nomarkdown}\( h_{n}\le1\){:/nomarkdown} לכל {::nomarkdown}\( n\){:/nomarkdown}. כל המידע הנוסף הזה יספיק לנו כדי לאתר את הפתרון המדויק בכל אחד מהמקרים.

כעת, אם {::nomarkdown}\( q&gt;p\){:/nomarkdown} (כלומר, הקזינו מוטה <strong>לרעת</strong> השחקן) אז {::nomarkdown}\( \left(\frac{q}{p}\right)^{n}\){:/nomarkdown} שואף לאינסוף כאשר {::nomarkdown}\( n\){:/nomarkdown} שואף לאינסוף. זה אומר שאלא אם {::nomarkdown}\( B=0\){:/nomarkdown}, כאשר נשאיף את {::nomarkdown}\( n\){:/nomarkdown} לאינסוף נקבל ערך לא הגיוני של {::nomarkdown}\( h_{n}\){:/nomarkdown} (או שיהיה קטן מאפס או שיהיה גדול מ-1). לכן {::nomarkdown}\( h_{n}=A\){:/nomarkdown} במקרה זה. אבל ידוע ש-{::nomarkdown}\( h_{0}=1\){:/nomarkdown} ולכן {::nomarkdown}\( A=1\){:/nomarkdown}. לכן הפתרון של נוסחת הנסיגה במקרה זה הוא "לא משנה מאיפה מתחילים, בהסתברות 1 נתרושש". לא מפתיע עד כדי כך בהתחשב בכך שכאן היתרון הוא של הקזינו.

אם {::nomarkdown}\( q&lt;p\){:/nomarkdown} היתרון הוא לצד השחקן הפעם ומכיוון ש-{::nomarkdown}\( \left(\frac{q}{p}\right)^{n}\){:/nomarkdown} שואף לאפס אי אפשר להשתמש בתעלול של קודם. אם כן, הבה וננסה להשתמש בתנאי ההתחלה {::nomarkdown}\( h_{0}=1\){:/nomarkdown} כדי לשפר קצת את יכולת הניתוח שלנו: {::nomarkdown}\( h_{0}=A+B\cdot\left(\frac{q}{p}\right)^{0}=A+B\){:/nomarkdown}, ולכן {::nomarkdown}\( B=1-A\){:/nomarkdown}, ולכן הפתרון הכללי הוא מהצורה {::nomarkdown}\( h_{n}=A+\left(1-A\right)\left(\frac{q}{p}\right)^{n}\){:/nomarkdown}. פתיחת סוגריים והוצאת גורם משותף ואנחנו מקבלים {::nomarkdown}\( h_{n}=\left(\frac{q}{p}\right)^{n}+A\left(1-\left(\frac{q}{p}\right)^{n}\right)\){:/nomarkdown}. כאשר {::nomarkdown}\( n\){:/nomarkdown} שואף לאינסוף הפתרון שואף ל-{::nomarkdown}\( A\){:/nomarkdown}, ולכן {::nomarkdown}\( 0\le A\){:/nomarkdown} בהכרח (אחרת היינו מקבלים שהחל ממקום כלשהו כל הפתרונות הם שליליים). אם כן, הפתרון הוא מהצורה "{::nomarkdown}\( \left(\frac{q}{p}\right)^{n}\){:/nomarkdown} ועוד משהו אי שלילי". אבל אמרנו שהפתרון הנכון הוא המינימלי, והמינימלי מתקבל כשאותו "משהו אי שלילי" שווה לאפס, ולכן הפתרון הוא {::nomarkdown}\( h_{n}=\left(\frac{q}{p}\right)^{n}\){:/nomarkdown}. המהלך האחרון - השימוש במינימליות - הוא ממש מקסים לטעמי.

אם כן, במקרה הנוכחי אנחנו רואים שהתכנסות לאפס איננה הכרחית, והיא דועכת אקספוננציאלית ככל שסכום הכסף שלנו גדל. במילים אחרות, הגעה לאפס כלל איננה מובנת מאליה; כאשר לקזינו יש יתרון, אנחנו מגיעים לאפס בצורה מאוד חזקה (הסתברות 1). כאשר לנו יש יתרון אנחנו מתרחקים מאפס בצורה חזקה למדי (דעיכה אקספוננציאלית לאפס של הסיכוי להגיע לאפס). קו הגבול בין שתי הסיטואציות הללו אינו שייך באופן מובהק לאף צד, אבל מחוסר הסימטריה של שתי התוצאות (הרי במקרה שבו לנו יש יתרון, עדיין זה לא אומר שהסתברות ההפסד שלנו היא 0) כבר אפשר לנחש שהיתרון יהיה של הקזינו גם במקרה ה"מאוזן". ואכן, זה מה שנחשב כרגע, אבל לפני כן אנחנו חייבים להתמודד עם בעיה כללית שצצה בפתרון נוסחאות נסיגה (וגם בפתרון משוואות דיפרנציאליות; הדמיון איננו מקרי) - מה קורה כשלמשוואה האופיינית יש שורש יחיד?

נתחיל מהסוף - אם למשוואה יש שורש יחיד {::nomarkdown}\( \lambda\){:/nomarkdown}, אז {::nomarkdown}\( a_{n}=\lambda^{n}\){:/nomarkdown} הוא פתרון אבל גם {::nomarkdown}\( a_{n}=n\lambda^{n}\){:/nomarkdown} הוא פתרון (להבדיל מהמקרה של שני שורשים). בואו נוכיח את זה, ובצורה כללית.

אז בואו ניקח נוסחת נסיגה {::nomarkdown}\( a_{n}=\alpha a_{n-1}+\beta a_{n-2}\){:/nomarkdown} (לאלו מכם שתוהים למה אני יכול להניח שהמקדם של {::nomarkdown}\( a_{n}\){:/nomarkdown} הוא 1 - אם הוא לא, פשוט נחלק בו, הוא לא יכול להיות 0). מנוסחת הנסיגה הזו נובעת המשוואה {::nomarkdown}\( x^{2}-\alpha x-\beta=0\){:/nomarkdown}; נניח שיש למשוואה הזו רק שורש יחיד, {::nomarkdown}\( \lambda\){:/nomarkdown}, אז עולה מכך השוויון {::nomarkdown}\( x^{2}-\alpha x-\beta=\left(x-\lambda\right)^{2}\){:/nomarkdown}. על ידי פתיחה והשוואת מקדמים מקבלים ש-{::nomarkdown}\( \alpha=2\lambda\){:/nomarkdown} ו-{::nomarkdown}\( \beta=-\lambda^{2}\){:/nomarkdown} (אלו הן בעצם <strong>נוסחאות וייטה</strong>). במילים אחרות, את נוסחת הנסיגה המקורית ניתן לכתוב כ-{::nomarkdown}\( a_{n}=2\lambda a_{n-1}-\lambda^{2}a_{n-2}\){:/nomarkdown}.

כעת, הבה ונבדוק מה קורה לפתרון המוצע שלנו, {::nomarkdown}\( a_{n}=n\lambda^{n}\){:/nomarkdown}, כאשר מציבים אותו במשוואה - באגף ימין מתקבל:

{::nomarkdown}\( 2\lambda\left(n-1\right)\lambda^{n-1}-\lambda^{2}\left(n-2\right)\lambda^{n-2}=\lambda^{n}\left[2n-2-n+2\right]=n\lambda^{n}\){:/nomarkdown}

וזו התוצאה הצפויה.

הייתי שמח לתת אינטואיציה יותר טובה פרט ל"תראו כמה יפה המשוואות מסתדרות", אבל אין לי - אולי למישהו אחר יש. השורה התחתונה היא שבמקרה של שורש יחיד למשוואה, יש לנו דרך אחרת (פשוטה ביותר) לקבל פתרון נוסף מתוך הפתרון הקיים - דרך שלא עובדת במקרה של שני שורשים.

חזרה לענייננו - אם הפתרון היחיד הוא {::nomarkdown}\( h_{n}=1\){:/nomarkdown} הרי שגם {::nomarkdown}\( h_{n}=n\){:/nomarkdown} יהיה פתרון, ולכן הפתרון הכללי למקרה שבו {::nomarkdown}\( p=q\){:/nomarkdown} יהיה {::nomarkdown}\( h_{n}=A+Bn\){:/nomarkdown}. כמקודם כך גם כאן זה מכריח את {::nomarkdown}\( B\){:/nomarkdown} להיות 0 אחרת עבור {::nomarkdown}\( n\){:/nomarkdown} גדול דיו נקבל פתרון גדול מ-1 או קטן מ-0. לכן הפתרון הוא {::nomarkdown}\( h_{n}=A\){:/nomarkdown} ומתנאי ההתחלה {::nomarkdown}\( h_{0}=1\){:/nomarkdown} נקבל {::nomarkdown}\( A=1\){:/nomarkdown}, כלומר הפתרון היחיד הוא שוב {::nomarkdown}\( h_{n}=1\){:/nomarkdown}, ולכן גם במקרה שבו {::nomarkdown}\( p=q\){:/nomarkdown}, לא משנה מה הסכום ההתחלתי שלך - בסוף תתרושש בודאות.

כאן מסתיים החלק המתמטי של הפוסט ואני יכול לשוב אל הטריגר לכתיבתו - מאמר של "הרב ד"ר מיכאל אברהם" באתר של Ynet שבו הוא מנסה לטעון שהאבולוציה דווקא מחזקת את האמונה בקיום אלוהים (ליתר דיוק, המאמר מוצג כקטע מספרו החדש של הרב ד"ר אברהם, אותו לא קראתי). המאמר נפתח ב:
<blockquote>האבולוציה היא תהליך שיש לו כיוון, מגמה. מחומר דומם והיולי, חסר כל תכונות, זה שנוצר במפץ הגדול, אנו מגיעים ליצורים חיים ומורכבים שנראים גם תכליתיים. סטיבן ג'יי גולד, שעומד על הקושי העקרוני הזה, מציע לו הסבר באמצעות <strong>משל השיכור</strong>.

הוא מסביר שייתכן מצב שבו תהליך שמתנהל בכיוונים אקראיים ובאופן לא מתוכנן יוליך לתוצאה הכרחית קבועה מראש, כמו הופעת יצורים מורכבים. כדי להבין זאת, אומר לנו גולד, עלינו לדמיין מצב שבו שיכור יוצא מהפאב אל המדרכה הרחבה שלפניו, משמאלו ניצב קיר הפאב ומימינו מדרכה, ומעבר למדרכה ישנה תעלת ביוב עמוקה.

השיכור מתחיל לזגזג פעם לכיוון הקיר ופעם לכיוון המדרכה, כלומר בכיוון אקראי. אך בסופו של דבר, אם ניתן לו די זמן, נמצא אותו, ללא ספק, מתגולל בתעלת הביוב. הקיר מייצג כמובן את האילוצים של הברירה הטבעית, והאקראיות בבחירת הצדדים היא היווצרות המוטציות. המשחק בין שני אלו יוצר את התוצאה הצפויה מראש.</blockquote>
לרוע המזל איני יודע לאיזה ספר של גולד הרב ד"ר אברהם מתייחס, כך שאיני יודע איך בדיוק גולד מתאר את משל השיכור; אבל למיטב הבנתי, זהו פשוט ניסוח אחר (שכבר שמעתי בעבר במקומות נוספים) של התרוששות המהמר, בהבדל אחד - <strong>לשיכור אין קיר﻿</strong>. השיכור מוזמן להתרחק מהתעלה עד כמה שירצה, זה לא משנה לו - כל עוד הוא פונה שמאלה או ימינה באותה הסתברות, הוא יתרושש - כלומר, ייפול לתעלה מתישהו. קיר, במקרה שלנו, רק משחק לרעתו, מכיוון שהוא מאלץ את השיכור להישאר תמיד במרחק חסום מהתעלה.

עם זאת, ולכן אני מצטער על כך שאיני יודע איך בדיוק גולד השתמש באנלוגיה הזו, השיכור-בתעלה היא אנלוגיה מסוכנת מכיוון שהיא דורשת אקראיות של 50-50. אם השיכור נוטה יותר ללכת לכיוון קיר הפאב מאשר לכיוון התעלה, לא מובטח לנו במאה אחוזים שהוא יגיע לתעלה. לכן לא צריך לקחת את השיכור בתור מודל מתמטי שבא לתאר את האבולוציה, אלא סתם בתור סיפור נחמד על "הנה סיטואציה שבה תהליך אקראי מתכנס באופן בטוח לחלוטין לתוצאה מסויימת; היי, גם האבולוציה, בדרכה שלה, היא כזו!".

אני לא יכול להתאפק מלהביא את הביקורת של הרב ד"ר אברהם על הטיעון של גולד למעלה, וזאת למרות שהביקורת הזו עוזבת לגמרי את המתרושש שלנו:
<blockquote>הבעייתיות במשל הזה, נעוצה בשתי רמות שונות: ראשית, הבעיה העיקרית היא שהקיר שלאורכו מתקדם השיכור עשוי מנייר. יתר על כן, מאחורי הקיר הזה יש תהום שכל באיה לא ישובון. כל הליכה לכיוון הלא שריד אבולוציונית (כלומר לכיוון הקיר) מוליכה לקצה מת (Dead end) של התהליך, כלומר להיכחדותו של היצור או של צאצאיו האבולוציוניים (נפילה של השיכור לתהום שמאחורי הקיר, במשל).

כל צעד אבולוציוני שיוצר מוטציה לא חיה מוליך להפסקה מוחלטת של התהליך, שקוטעת את כל השרשרת האבולוציונית. בתהליך האבולוציוני אין לנו קיר שדואג לזה שבכל פעם השיכור ישוב מחדש למסלול. אם באחד מהשלבים במשך ההיסטוריה לא נוצרת מוטציה שרידה, התהליך כולו נפסק סופית.

אם כן, במשל המתוקן, הקיר שלאורך המדרכה עשוי מנייר. כל פעם שהשיכור נופל עליו הוא קורע אותו ונופל לתהום שמאחוריו. כעת אפשר לשער: מה הסיכוי, במקרה כזה, שהשיכור המתנהל באקראי יגיע בסופו של דבר לתעלת הביוב?</blockquote>
על הטיעון הזה אני יכול להגיד - או שאני לא מבין כלום באבולוציה, או שהרב ד"ר אברהם לא מבין כלום באבולוציה (או ששנינו לא מבינים בה כלום). אנסה להסביר כמיטב הבנתי (וצריך לקחת בחשבון שאכן איני מבין כלום באבולוציה, שכן אינני ביולוג ולא למדתי את הנושא בצורה מסודרת מעולם).

אם כן, אין ספק שצעד אבולוציוני שיוצר "מוטציה לא חיה" מוביל להפסקת התהליך מבחינת המוטציה המסכנה הזו. האם הוא עוצר את התהליך האבולוציוני כולו? ודאי שלא. הבעיה כאן היא שהרב ד"ר אברהם מנסה לטעון שבאבולוציה, כל אינדיבידואל הוא שיכור שמתהלך לו לבד; ואילו גולד (בכך אני בטוח לחלוטין) חושב על השיכור לכל הפחות כעל אוכלוסייה של מין כלשהו. אם עוברים לרזולוציה הזו, הטיעון של הרב ד"ר אברהם כבר לא תקף; זה שחלק מהפרטים באוכלוסייה יוולדו עם פגם גנטי שיגרום להם להיוולד מתים זה עצוב וטרגי אבל כל עוד הם היוצאים מן הכלל (ומכיוון שהם מוטציות, הם היוצאים מן הכלל) הם לא יגרמו למות האוכלוסייה כולה. כאן מתגלה דווקא כוחו של התהליך הזה: פרטים שנולדים עם מוטציה "גרועה" שפוגעת בשרידות שלהם, קרוב לודאי שלא יזכו להעביר את המוטציה הזו הלאה ולהפיץ אותה בקרב האוכלוסייה. כלומר, המוטציות שכן מתפשטות באוכלוסייה הן כאלו ש<strong>אינן פוגעת ביכולת העברת המוטציה הלאה</strong>. כדאי לזכור שרוב המוטציות לא מתבטאות לא לחיוב ולא לשלילה באופן מיידי, כך שכבר ברור שמודל השיכור (שבו כל צעד או שמקדם או שמרחיק אותנו) הוא לא באמת תיאור טוב במיוחד לכל הסיפור.

השיכור של הרב ד"ר אברהם יפול לתהום שכל באיה לא ישובון רק אם באוכלוסייה תתפשט מוטציה כזו שמצד אחד משפרת את יכולת ההתרבות של הפרטים שיש להם אותה, ומצד שני "תוקעת" אותם. אני יכול להעלות בדמיוני מוטציות כאלו - למשל, המוטציה "תחייה חיי נצח אבל כשתתרבה לא יהיו לך מוטציות", שמצד אחד משפרת מאוד את יכולת השרידות של החיה אבל מצד שני מונעת כל שינוי מכאן והלאה (זה לא הורג את הפרט, אבל זה כן מכניס את האבולוציה של האוכלוסיה למבוי סתום). זו כמובן מוטציה מגוחכת לחלוטין והביולוגים יהרגו אותי על כך שהזכרתי אותה ובוודאי יש דוגמאות יותר טובות שאיני חושב עליהן. בכל מקרה לא נראה לי שהרב ד"ר אברהם מדבר על זה בכלל אלא דווקא על מוטציות כושלות שהורגות את הפרטים שלוקים בהן (כמובן שבעולמנו האכזר יש עוד דברים פרט למוטציות שיכולים להשמיד לחלוטין אוכלוסיות - תשאלו את הדינוזאורים - אבל גם זה לא טיעון שהרב ד"ר אברהם העלה בכלל וזה בוודאי לא קשור להילוך השיכור).

בהמשך הרב ד"ר אברהם מביא עוד טיעונים שלדעתי אפילו יותר מבורחשים וממילא כבר מאבדים כל קשר למתמטיקה שבבסיס הפוסט הזה ולכן לא אמשיך להתייחס אליהם. רק על דבר אחד נוסף אני חייב להתלונן - בחירת הכותרת של העורך של Ynet. אז נכון, הרב ד"ר אברהם בחר לקרוא לספרו "אלוהים משחק בקוביות", אבל לתת כותרת של "האם איינשטיין טעה?" למאמר שכלל אינו מדבר על תורת הקוונטיים, שהיא ההקשר שבו איינשטיין אמר את הציטוט המפורסם שלו על הקוביות - ובכן, זה כבר עובר את גבול ה"מביך ברמות בלתי נתפסות" ויוצא מהצד השני.
