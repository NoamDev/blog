---
id: 3273
title: "אנליזה וקטורית - מבוא"
date: 2015-07-02 17:03:32
layout: post
categories: 
  - אנליזה מתמטית
tags: 
  - אנליזה וקטורית
---
בזמנו כתבתי בבלוג <a href="http://www.gadial.net/2010/09/05/calculus_intro/">סדרת פוסטים</a> על חשבון אינפיניטסימלי. הצגתי בסדרה הזו את שלושת המושגים הבסיסיים שעליהם החשבון האינפיניטסימלי נשען - הגבול, הנגזרת והאינטגרל - אבל לא הלכתי יותר מדי רחוק אחר כך (אפילו לנושא כמו טורים לא הגעתי).

בעקבות בקשה שהגיעה אלי, אני רוצה להתחיל להשלים חלק מהחורים שהשארתי. ספציפית, אני רוצה לטפל באנליזה וקטורית (אצלי "אנליזה וקטורית" היא דרך מקוצרת לומר "חשבון אינפיניטסימלי במספר משתנים; אני מודע לכך שלפעמים משתמשים בשם הזה כדי לתאר תחום ספציפי יותר). הרעיון הבסיסי של התחום הוא - יופי, אז יש לנו חשבון אינפיניטסימלי של פונקציות {% equation %}f:\mathbb{R}\to\mathbb{R}{% endequation %} - פונקציות ממשיות במשתנה יחיד. זה לכשעצמו כבר תחום חזק ומועיל ביותר, אבל בעולם האמיתי יש לנו הרבה פונקציות מורכבות יותר - כאלו שמקבלות כמה משתנים, ואפילו מחזירות כמה ערכים; באופן כללי, פונקציות {% equation %}f:\mathbb{R}^{n}\to\mathbb{R}^{m}{% endequation %}. האם ניתן להכליל את מושגי הגבול, הנגזרת והאינטגרל גם עבורם? התשובה היא כמובן "כן", והתוצאה היא אחד התחומים החשובים ביותר במתמטיקה, וכזה שיש לו אינסוף שימושים יישומיים (למשל, בלי התחום הזה אין פיזיקה).

כפי שאתם ודאי מנחשים, אני הולך להניח שאתם כבר מכירים חשבון אינפיניטסימלי של פונקציות ממשיות במשתנה יחיד. אם אתם לא מכירים - נסו לקרוא את סדרת הפוסטים שלי (או לקחת ספר לימוד). אולי קצת פחות ברור כרגע הוא תחום נוסף שאני אניח שאתם כבר מכירים - אלגברה לינארית בסיסית. הסיבות לכך יתבררו עוד מעט.

בפוסט הזה אני רוצה לתת סקירה כללית של הנושאים שאנחנו מדברים עליהם, בלי להיכנס יותר מדי לפורמליזם. ראשית נתחיל עם הצגת שדה המשחק, שהוא כאמור פונקציות {% equation %}f:\mathbb{R}^{n}\to\mathbb{R}^{m}{% endequation %} מכיוון שזו דרך הצגה מאוד כללית, הרבה פעמים כשאני ארצה לתת דוגמה קונקרטית, אני אלך למקרה הלא טריוויאלי הראשון: פונקציה ממשית בשני משתנים, {% equation %}f:\mathbb{R}^{2}\to\mathbb{R}{% endequation %}. למשל {% equation %}f\left(x,y\right)=x+y{% endequation %} או {% equation %}f\left(x,y\right)=\sin x\cos y{% endequation %} וכדומה. ויזואליזציה נחמדה לפונקציה שכזו היא בתור <strong>גובה פני השטח</strong> במפה: לכל קואורדינטת {% equation %}\left(x,y\right){% endequation %} מותאם גובה {% equation %}f\left(x,y\right){% endequation %} שהוא מספר ממשי. זה נראה ככה (תמונה באדיבות ויקיפדיה האנגלית):

<strong><a href="{{site.baseurl}}{{site.post_images}}/2015/07/Three-dimensional_graph.png"><img class="aligncenter size-full wp-image-3274" alt="Three-dimensional_graph" src="{{site.baseurl}}{{site.post_images}}/2015/07/Three-dimensional_graph.png" width="855" height="706" /></a></strong>

כלומר, מין הר שכזה. עכשיו אפשר לנסח שתי שאלות על היצור הזה: ראשית, נניח שאנחנו נמצאים בנקודה מסויימת; כמה <strong>תלול</strong> ההר באותה נקודה? שנית, בואו ניקח שטח תחום כלשהו מההר - מה ה<strong>נפח</strong> שלו? כלומר, כמה סלעים יש בפנים? על השאלה הראשונה עונים באמצעות נגזרות, ועל השאלה השניה באמצעות אינטגרלים. רק שהסיפור יותר מסובך כאן מאשר במימד אחד. למשל, הביטו על האוכף הזה (שוב, תודה לויקיפדיה האנגלית):

<strong><a href="{{site.baseurl}}{{site.post_images}}/2015/07/Saddle_point.png"><img class="aligncenter size-full wp-image-3275" alt="Saddle_point" src="{{site.baseurl}}{{site.post_images}}/2015/07/Saddle_point.png" width="978" height="813" /></a></strong>

במרכז האוכף, מה קורה? האם אנחנו עולים או יורדים? ובאיזה קצב? נראה שזה תלוי ב<strong>כיוון</strong> שאנחנו הולכים בו. כבר אי אפשר לדבר על נגזרת רגילה, צריך לדבר על "נגזרת עבור כיוון מסויים" - <strong>נגזרת מכוונת</strong>. אבל מתברר שיש גם דרך לדבר בצורה כלשהי על "הנגזרת לכל הכיוונים בבת אחת", פשוט לא נקבל מספר כמו קודם אלא <strong>טרנספורמציה לינארית</strong>. מעט בעייתי להסביר את זה בלי להיכנס לפורמליזם (שאכנס אליו בפוסט הבא), אבל זהו הרעיון: בפונקציות ממשיות במשתנה יחיד, הנגזרת של פונקציה {% equation %}f{% endequation %} בנקודה מסויימת הייתה בסך הכל מספר ממשי ("המספר הנגזר"). המספר הזה נתן לנו את "קצב השינוי הרגעי" של הפונקציה באותה נקודה, אבל יש גם דרך התבוננות גאומטרית על זה: הנגזרת בנקודה נתנה לנו את <strong>שיפוע המשיק</strong> לגרף הפונקציה באותה נקודה. אבל מה זה "משיק"? זה קו ישר שעובר דרך הנקודה הזו ו"נראה בערך כמו הפונקציה" באותה נקודה. עכשיו בואו ניקח את הרעיון הזה צעד קדימה. קו ישר הוא <strong>טרנספורמציה לינארית</strong>: הוא פונקציה מהצורה {% equation %}T\left(x\right)=ax+b{% endequation %}, ואנחנו יכולים בלי הגבלת הכלליות להתייחס לנקודה שבה אנו מחשבים את הפונקציה בתור ראשית הצירים כדי לקבל ש-{% equation %}b=0{% endequation %}, כלומר {% equation %}T\left(x\right)=ax{% endequation %} כאשר {% equation %}a{% endequation %} הוא המספר הנגזר של {% equation %}f{% endequation %} בנקודה 0 הזו ({% equation %}a=f^{\prime}\left(0\right){% endequation %}). אפשר להוכיח שבמובן מסויים, {% equation %}T{% endequation %} הזו מהווה קירוב טוב ל-{% equation %}f{% endequation %} בסביבות הנקודה 0. לקירוב הזה קוראים <strong>דיפרנציאל</strong> של {% equation %}f{% endequation %} בנקודה 0.

מה שעושים באנליזה וקטורית הוא להכליל בראש ובראשונה את מושג הדיפרנציאל הזה. אם {% equation %}f:\mathbb{R}^{n}\to\mathbb{R}^{m}{% endequation %} אז הדיפרנציאל של {% equation %}f{% endequation %} בנקודה מסויימת יהיה טרנספורמציה לינארית {% equation %}T:\mathbb{R}^{n}\to\mathbb{R}^{m}{% endequation %} שמהווה קירוב טוב ל-{% equation %}f{% endequation %} באותה נקודה.

פרט למושג הדיפרנציאל, ניתן לדבר על מה שתיארתי קודם - נגזרות מכוונות. בפרט, נגזרות שמכוונות בכיוון הצירים של המשתנים. נגזרות כאלו נקראות <strong>נגזרות חלקיות</strong> של {% equation %}f{% endequation %} והן חשובות בפני עצמן; בדרך כלל הן קשורות בקשר הדוק לדיפרנציאלים של הפונקציה, אבל ייתכנו דוגמאות פתולוגיות שבהן יש לפונקציה דיפרנציאל אבל אין נגזרות חלקיות, וההפך.

בשלב הזה אפשר להקים לתחיה דיונים מאינפי של פונקציות ממשיות - איך מחשבים את הדיפרנציאל הזה? (בעזרת אינפי של פונקציות ממשיות). האם מתקיים כלל השרשרת? (כן). האם אפשר להשתמש בו כדי למצוא נקודות מינימום ומקסימום? (כן). האם קיימת הכללה של קירוב טיילור עבור פונקציה בכמה משתנים? (כן). ויש גם דיון שבפונקציות ממשיות רגילות הוא כמעט טריוויאלי והופך למורכב ומעניין פי כמה וכמה בהקשר הזה - באילו תנאים קיימת <strong>הופכית</strong> מקומית לפונקציה דיפרנציאבילית? המשפט הרלוונטי (<strong>משפט הפונקציה ההפוכה</strong>) הוא ההוכחה ה"כבדה" הראשונה שצצה בתחום הזה, ומייד אפשר לתת לה שימוש יפה ב<strong>משפט הפונקציות הסתומות</strong>, שהוא משהו שאין לו מקבילה באינפי של משתנה יחיד, שעוסק בשאלת היכולת שלנו "לחלץ" משתנה מתוך ביטוי; למשל, המשוואה {% equation %}x^{2}+y^{2}=1{% endequation %} מגדירה בצורה כלשהי את {% equation %}x{% endequation %} כפונקציה של {% equation %}y{% endequation %}, כפי שאפשר לראות על ידי החילוץ לדוגמה {% equation %}x=\sqrt{1-y^{2}}{% endequation %} - אבל האם תמיד אפשר לבצע חילוץ שכזה? והאם הוא יחיד? (במקרה שלנו, כמובן, גם {% equation %}x=-\sqrt{1-y^{2}}{% endequation %} עובד) - זה מה שמשפט הפונקציות הסתומות מתעסק בו.

ואז מגיעים לאינטגרלים והכל משתגע.

יש שתי הצגות של מה שאדבר עליו עכשיו. אתחיל מזו ה"קלאסית" ואז אזכיר ברמז את המודרנית יותר. אני מקווה שגם בפוסטים העניינים יתנהלו כך.

על פניו, ההכללה של אינטגרל רימן לממדים גבוהים יותר היא טבעית יחסית. אינטגרל רימן התקבל מסדרת קירובים באמצעות מלבנים? אז עכשיו ניקח קירובים באמצעות קוביות וכן הלאה. זה אכן הרעיון הבסיסי, והוא מניב בדו ותלת מימד את מה שנקראים "אינטגרל כפול" ו"אינטגרל משולש", בהתאמה (ואפשר לדבר גם על ממדים גבוהים יותר). אבל חיש קל עולה השאלה איך מחשבים דבר כזה - והתשובה היא שבאמצעות אינטגרל "רגיל", כמובן; זה מה שנקרא <strong>משפט פוביני</strong>, שמראה איך אפשר (לרוב, לא תמיד) לחשב אינטגרל כפול באמצעות חישוב שני אינטגרלים רגילים, ואינטגרל משולש באמצעות חישוב שלושה - הבנתם את הרעיון.

זה עדיין לא מסיים את העניין, כי טכניקת אינטגרציה נפוצה מאוד - החלפת משתנים - הופכת לקריטית עוד יותר בממדים גבוהים. המשפט שמטפל באופן שבו ניתן לבצע החלפת משתנים הוא עוד תוצאה כבדה למדי.

אבל האינטגרלים לא נגמרים כאן. אפשר לעבור לדבר על סוג אחר של אינטגרלים - <strong>אינטגרל קווי</strong> ו<strong>אינטגרל משטחי</strong>. וכאילו שאין די בצרות, יש <strong>שני סוגים</strong> של כל אחד מהאינטגרלים הללו. נדבר על אינטגרל קווי: הסוג הראשון עוסק בסיטואציה שבה יש לנו פונקציה סקלרית כלשהי על מרחב {% equation %}n{% endequation %}-ממדי ({% equation %}f:\mathbb{R}^{n}\to\mathbb{R}{% endequation %}) ואנחנו לא רוצים לחשב אינטגרל דו-ממדי שלה, אלא רוצים לשאול את השאלה מה האינטגרל ה"רגיל" שלה כשהוא נלקח על אובייקט חד ממדי - עקומה כלשהי שחיה ב-{% equation %}\mathbb{R}^{n}{% endequation %}. הסוג השני הוא אינטגרל של פונקציה <strong>וקטורית</strong>, כלומר פונקציה {% equation %}F:\mathbb{R}^{n}\to\mathbb{R}^{n}{% endequation %}. דוגמה אינטואיטיבית מאוד לעניין הזה, למי שמכיר פיזיקה, היא <strong>עבודה</strong>: אם אנחנו רוצים לחשב את השינוי באנרגיה הקינטית של גוף שנע בתוך שדה כוח כלשהו (זו הפונקציה {% equation %}F{% endequation %}) במסלול מסויים (זו העקומה שעליה מבצעים את האינטגרל) צריך לחשב את האינטגרל הקווי של {% equation %}F{% endequation %} על העקומה. עבור אינטגרל משטחי, הסיפור דומה, אבל במקום אינטגרציה על עקומה חד ממדית, אנחנו רוצים לבצע אינטגרציה על משטח דו ממדי.

ואז כדי לסיים את זה מגיעים כמה משפטים כבדים שמראים כל מני קשרים בין סוגי האינטגרלים הללו - <strong>משפט גרין, משפט גאוס, ומשפט סטוקס</strong>. בנפנוף ידיים פרוע, משפט גרין מראה שב-{% equation %}\mathbb{R}^{2}{% endequation %}, חישוב של אינטגרל כפול של פונקציה על תחום מסויים זהה לחישוב של אינטגרל קווי של פונקציה אחרת שמתקבלת ממנה, על <strong>השפה</strong> של אותו תחום. משפט גאוס עושה את אותו הדבר עבור {% equation %}\mathbb{R}^{3}{% endequation %}, עם המרה בין אינטגרל משולש ואינטגרל משטחי על שפת התחום שבו מבצעים את האינטגרציה המשולשת. משפט סטוקס נשאר ב-{% equation %}\mathbb{R}^{3}{% endequation %} ומראה קשר דומה בין אינטגרל משטחי ובין אינטגרל קווי על השפה של המשטח.

כל העסק הזה נראה כמו בלאגן אחד גדול, והוא אכן כזה; למרבה המזל, המתמטיקה המודרנית מגיעה לעזרה. כדי להכליל את כל ערב-רב המושגים שתיארתי קודם, מכניסים לתמונה מושג שנקרא <strong>תבנית דיפרנציאלית</strong>, ובעזרתו אפשר להוכיח משפט שממנו גרין, גאוס וסטוקס הולכים להיגזר כמקרים פרטיים. המשפט הכללי נקרא (באופן ממש לא מבלבל) <strong>משפט סטוקס</strong> (זה "ה"משפט סטוקס; משפט סטוקס שתיארתי קודם נקרא "חוק סטוקס" או "משפט סטוקס הקלאסי" וכדומה כדי להבדיל ביניהם). הניסוח שלו הוא מאוד פשוט ונקי:

{% equation %}\int_{M}d\omega=\int_{\partial M}\omega{% endequation %}

צריך לקרוא את זה כך: אם {% equation %}\omega{% endequation %} היא תבנית דיפרנציאלית ו-{% equation %}M{% endequation %} היא תחום כלשהו שמקיים כך וכך ("יריעה דיפרנציאלית קומפקטית מכוונת") אז האינטגרל של {% equation %}\omega{% endequation %} על שפת {% equation %}M{% endequation %} שווה לאינטגרל של הנגזרת של {% equation %}\omega{% endequation %} על {% equation %}M{% endequation %}.

כמובן, בלי להבין את המושגים שנכללים בהגדרת המשפט אי אפשר להבין עד הסוף מה הולך כאן, אבל אני חושב שאפשר להעריך את הפשטות של התיאור הזה כבר כעת.

משפט סטוקס הזה הוא סוג של גביע קדוש בתחום - הוא מהווה הכללה של <strong>המשפט היסודי של החדו"א</strong>. גם בלי להבין את המושגים עד הסוף, אפשר לראות את זה די בקלות: באינטגרל "רגיל", התחום {% equation %}M{% endequation %} שלנו הוא קטע {% equation %}\left[a,b\right]{% endequation %}. ה"שפה" של תחום כזה היא בסך הכל הנקודות {% equation %}\left\{ a,b\right\} {% endequation %}. אם {% equation %}F{% endequation %} היא פונקציה כלשהי שאנחנו חושבים עליה כעל תבנית דיפרנציאלית (תבנית דיפרנציאלית היא הכללה של פונקציות), אז {% equation %}dF=f{% endequation %} היא הנגזרת הרגילה שלה. לכן אנחנו מקבלים ש-{% equation %}\int_{a}^{b}f{% endequation %} שווה ל"אינטגרל" של {% equation %}F{% endequation %} על הנקודות {% equation %}\left\{ a,b\right\} {% endequation %} - מה שיוצא {% equation %}F\left(b\right)-F\left(a\right){% endequation %}. למה אחד מהם הוא חיובי והשני שלילי? זה נובע מהכיווניות של ה"יריעה" שכוללת את שתי הנקודות הללו; אסביר את הפרטים הטכניים כשאגיע אליהם, בתקווה.

אם כן, זה הרעיון הכללי ואלו הדברים שאני מקווה שיצא לי להציג (גם אם לא ברצף, ראו בפוסט הזה התחייבות כלשהי לעתיד). אני מקווה שיהיה מעניין לפחות כשם שיהיה שימושי.
